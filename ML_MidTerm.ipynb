{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQPNR08rglpt"
      },
      "source": [
        "<div class=\"alert alert-block alert-success\">\n",
        "<h1>Machine Learning MidTermProject<h1>\n",
        "Amin Abdipour 401133011</h1>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run for first time"
      ],
      "metadata": {
        "id": "LbP0zDHVzwK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install tqdm scipy resampy xgboost librosa\n",
        "# !wget -r -N -c -np https://physionet.org/files/circor-heart-sound/1.0.3/\n",
        "# import shutil\n",
        "# shutil.move('physionet.org', '/content/drive/My Drive/ML')"
      ],
      "metadata": {
        "id": "TxX10AYc5qUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQA7xuX1i9mZ",
        "outputId": "eb6c44fd-1da6-4ae7-b2da-21e21dc94706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCzkV-6xVs6g"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import os.path as op\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = op.join('/content/drive/My Drive/','ML','physionet.org')  # Path to the Data folder"
      ],
      "metadata": {
        "id": "84qGsktL0N2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attachs\n"
      ],
      "metadata": {
        "id": "SZLXRyUBtE6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## helper code"
      ],
      "metadata": {
        "id": "W1-CPjSIuIXc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "import scipy.io.wavfile\n",
        "import scipy.signal\n",
        "\n",
        "\n",
        "# Check if a variable is a number or represents a number.\n",
        "def is_number(x):\n",
        "    try:\n",
        "        float(x)\n",
        "        return True\n",
        "    except (ValueError, TypeError):\n",
        "        return False\n",
        "\n",
        "\n",
        "# Check if a variable is an integer or represents an integer.\n",
        "def is_integer(x):\n",
        "    if is_number(x):\n",
        "        return float(x).is_integer()\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Check if a variable is a finite number or represents a finite number.\n",
        "def is_finite_number(x):\n",
        "    if is_number(x):\n",
        "        return np.isfinite(float(x))\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "# Compare normalized strings.\n",
        "def compare_strings(x, y):\n",
        "    try:\n",
        "        return str(x).strip().casefold() == str(y).strip().casefold()\n",
        "    except AttributeError:  # For Python 2.x compatibility\n",
        "        return str(x).strip().lower() == str(y).strip().lower()\n",
        "\n",
        "\n",
        "# Load a WAV file.\n",
        "def load_wav_file(filename):\n",
        "    frequency, recording = scipy.io.wavfile.read(filename)\n",
        "    return recording, frequency\n",
        "\n",
        "\n",
        "# Load recordings.\n",
        "def load_recordings(data_folder, data, get_frequencies=False):\n",
        "    num_locations = get_num_locations(data)\n",
        "    recording_information = data.split(\"\\n\")[1 : num_locations + 1]\n",
        "\n",
        "    recordings = list()\n",
        "    frequencies = list()\n",
        "    for i in range(num_locations):\n",
        "        entries = recording_information[i].split(\" \")\n",
        "        recording_file = entries[2]\n",
        "        filename = os.path.join(data_folder, recording_file)\n",
        "        recording, frequency = load_wav_file(filename)\n",
        "        recordings.append(recording)\n",
        "        frequencies.append(frequency)\n",
        "\n",
        "    if get_frequencies:\n",
        "        return recordings, frequencies\n",
        "    else:\n",
        "        return recordings\n",
        "\n",
        "\n",
        "# Get number of recording locations from patient data.\n",
        "def get_num_locations(data):\n",
        "    num_locations = None\n",
        "    for i, l in enumerate(data.split(\"\\n\")):\n",
        "        if i == 0:\n",
        "            num_locations = int(l.split(\" \")[1])\n",
        "        else:\n",
        "            break\n",
        "    return num_locations\n",
        "\n",
        "\n",
        "# Get recording locations from patient data.\n",
        "def get_locations(data):\n",
        "    num_locations = get_num_locations(data)\n",
        "    locations = list()\n",
        "    for i, text in enumerate(data.split(\"\\n\")):\n",
        "        entries = text.split(\" \")\n",
        "        if i == 0:\n",
        "            pass\n",
        "        elif 1 <= i <= num_locations:\n",
        "            locations.append(entries[0])\n",
        "        else:\n",
        "            break\n",
        "    return locations\n",
        "\n",
        "\n",
        "# Sanitize binary values from Challenge outputs.\n",
        "def sanitize_binary_value(x):\n",
        "    x = (\n",
        "        str(x).replace('\"', \"\").replace(\"'\", \"\").strip()\n",
        "    )  # Remove any quotes or invisible characters.\n",
        "    if (is_finite_number(x) and float(x) == 1) or (x in (\"True\", \"true\", \"T\", \"t\")):\n",
        "        return 1\n",
        "    else:\n",
        "        return 0"
      ],
      "metadata": {
        "id": "CiyBa3USuJ_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataProcessing.find_and_load_patient_files"
      ],
      "metadata": {
        "id": "Pr8WJLl0top1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# from DataProcessing.helper_code import is_integer\n",
        "\n",
        "\n",
        "# Find patient data files.\n",
        "def find_patient_files(data_folder):\n",
        "    # Find patient files.\n",
        "    filenames = list()\n",
        "    for f in sorted(os.listdir(data_folder)):\n",
        "        root, extension = os.path.splitext(f)\n",
        "        if not root.startswith(\".\") and extension == \".txt\":\n",
        "            filename = os.path.join(data_folder, f)\n",
        "            filenames.append(filename)\n",
        "\n",
        "    # To help with debugging, sort numerically if the filenames are integers.\n",
        "    roots = [os.path.split(filename)[1][:-4] for filename in filenames]\n",
        "    if all(is_integer(root) for root in roots):\n",
        "        filenames = sorted(\n",
        "            filenames, key=lambda filename: int(os.path.split(filename)[1][:-4])\n",
        "        )\n",
        "\n",
        "    return filenames\n",
        "\n",
        "\n",
        "# Load patient data as a string.\n",
        "def load_patient_data(filename):\n",
        "    with open(filename, \"r\") as f:\n",
        "        data = f.read()\n",
        "    return data"
      ],
      "metadata": {
        "id": "Fc21GrHPt5nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Label extraction"
      ],
      "metadata": {
        "id": "Girc5DwSuXyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get murmur from patient data.\n",
        "def get_murmur(data):\n",
        "    murmur = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Murmur:\"):\n",
        "            murmur = text.split(\": \")[1]\n",
        "    if murmur is None:\n",
        "        raise ValueError(\n",
        "            \"No murmur available. Is your code trying to load labels from the hidden data?\"\n",
        "        )\n",
        "    return murmur\n",
        "\n",
        "\n",
        "# Get outcome from patient data.\n",
        "def get_outcome(data):\n",
        "    outcome = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Outcome:\"):\n",
        "            outcome = text.split(\": \")[1]\n",
        "    if outcome is None:\n",
        "        raise ValueError(\n",
        "            \"No outcome available. Is your code trying to load labels from the hidden data?\"\n",
        "        )\n",
        "    return outcome"
      ],
      "metadata": {
        "id": "IrHEBrAnudeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataProcessing.XGBoost_features.metadata"
      ],
      "metadata": {
        "id": "xc6YTYbouru7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# from DataProcessing.helper_code import compare_strings, sanitize_binary_value\n",
        "\n",
        "\n",
        "# Get age from patient data.\n",
        "def get_age(data):\n",
        "    age = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Age:\"):\n",
        "            age = text.split(\": \")[1].strip()\n",
        "    return age\n",
        "\n",
        "\n",
        "# Get sex from patient data.\n",
        "def get_sex(data):\n",
        "    sex = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Sex:\"):\n",
        "            sex = text.split(\": \")[1].strip()\n",
        "    return sex\n",
        "\n",
        "\n",
        "# Get height from patient data.\n",
        "def get_height(data):\n",
        "    height = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Height:\"):\n",
        "            height = float(text.split(\": \")[1].strip())\n",
        "    return height\n",
        "\n",
        "\n",
        "# Get weight from patient data.\n",
        "def get_weight(data):\n",
        "    weight = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Weight:\"):\n",
        "            weight = float(text.split(\": \")[1].strip())\n",
        "    return weight\n",
        "\n",
        "\n",
        "# Get pregnancy status from patient data.\n",
        "def get_pregnancy_status(data):\n",
        "    is_pregnant = None\n",
        "    for text in data.split(\"\\n\"):\n",
        "        if text.startswith(\"#Pregnancy status:\"):\n",
        "            is_pregnant = bool(sanitize_binary_value(text.split(\": \")[1].strip()))\n",
        "    return is_pregnant\n",
        "\n",
        "\n",
        "# Extract features from the data.\n",
        "def get_metadata(data):\n",
        "\n",
        "    # Extract the age group and replace with the (approximate) number of months\n",
        "    # for the middle of the age group.\n",
        "    age_group = get_age(data)\n",
        "\n",
        "    if compare_strings(age_group, \"Neonate\"):\n",
        "        age = 0.5\n",
        "    elif compare_strings(age_group, \"Infant\"):\n",
        "        age = 6\n",
        "    elif compare_strings(age_group, \"Child\"):\n",
        "        age = 6 * 12\n",
        "    elif compare_strings(age_group, \"Adolescent\"):\n",
        "        age = 15 * 12\n",
        "    elif compare_strings(age_group, \"Young Adult\"):\n",
        "        age = 20 * 12\n",
        "    else:\n",
        "        age = float(\"nan\")\n",
        "\n",
        "    # Extract sex. Use one-hot encoding.\n",
        "    sex = get_sex(data)\n",
        "\n",
        "    sex_features = np.zeros(2, dtype=int)\n",
        "    if compare_strings(sex, \"Female\"):\n",
        "        sex_features[0] = 1\n",
        "    elif compare_strings(sex, \"Male\"):\n",
        "        sex_features[1] = 1\n",
        "\n",
        "    # Extract height and weight.\n",
        "    height = get_height(data)\n",
        "    weight = get_weight(data)\n",
        "\n",
        "    # Extract pregnancy status.\n",
        "    is_pregnant = get_pregnancy_status(data)\n",
        "\n",
        "    features = np.hstack(([age], sex_features, [height], [weight], [is_pregnant]))\n",
        "\n",
        "    return np.asarray(features, dtype=np.float32)"
      ],
      "metadata": {
        "id": "he0JfWq4urFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## net_feature_extractor.py"
      ],
      "metadata": {
        "id": "Fn8svHr8HkdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from DataProcessing.find_and_load_patient_files import (\n",
        "    find_patient_files,\n",
        "    load_patient_data,\n",
        ")\n",
        "from DataProcessing.helper_code import get_num_locations, load_wav_file\n",
        "from DataProcessing.label_extraction import get_murmur, get_outcome\n",
        "from HumBugDB.LogMelSpecs.compute_LogMelSpecs import waveform_to_examples\n",
        "\n",
        "\n",
        "def net_feature_loader(\n",
        "    recalc_features, train_data_directory, test_data_directory, spectrogram_directory\n",
        "):\n",
        "\n",
        "    if not os.path.isdir(spectrogram_directory):\n",
        "        os.makedirs(spectrogram_directory)\n",
        "    if recalc_features:\n",
        "        spectrograms_train, murmurs_train, outcomes_train = calc_patient_features(\n",
        "            train_data_directory\n",
        "        )\n",
        "        repeats = torch.zeros((len(spectrograms_train),))\n",
        "        for i in range(len(spectrograms_train)):\n",
        "            for j in range(len(spectrograms_train[i])):\n",
        "                repeats[i] += len(spectrograms_train[i][j])\n",
        "        murmurs_train = torch.repeat_interleave(\n",
        "            torch.Tensor(np.array(murmurs_train)), repeats.to(torch.int32), dim=0\n",
        "        )\n",
        "        outcomes_train = torch.repeat_interleave(\n",
        "            torch.Tensor(np.array(outcomes_train)), repeats.to(torch.int32), dim=0\n",
        "        )\n",
        "        spectrograms_train = torch.cat([x for xs in spectrograms_train for x in xs])\n",
        "        torch.save(\n",
        "            spectrograms_train, os.path.join(spectrogram_directory, \"spec_train\")\n",
        "        )\n",
        "        torch.save(murmurs_train, os.path.join(spectrogram_directory, \"murmurs_train\"))\n",
        "        torch.save(\n",
        "            outcomes_train, os.path.join(spectrogram_directory, \"outcomes_train\")\n",
        "        )\n",
        "\n",
        "        spectrograms_test, murmurs_test, outcomes_test = calc_patient_features(\n",
        "            test_data_directory\n",
        "        )\n",
        "        repeats = torch.zeros((len(spectrograms_test),))\n",
        "        for i in range(len(spectrograms_test)):\n",
        "            for j in range(len(spectrograms_test[i])):\n",
        "                repeats[i] += len(spectrograms_test[i][j])\n",
        "        murmurs_test = torch.repeat_interleave(\n",
        "            torch.Tensor(np.array(murmurs_test)), repeats.to(torch.int32), dim=0\n",
        "        )\n",
        "        outcomes_test = torch.repeat_interleave(\n",
        "            torch.Tensor(np.array(outcomes_test)), repeats.to(torch.int32), dim=0\n",
        "        )\n",
        "        spectrograms_test = torch.cat([x for xs in spectrograms_test for x in xs])\n",
        "        murmurs_test = torch.Tensor(np.array(murmurs_test))\n",
        "        outcomes_test = torch.Tensor(np.array(outcomes_test))\n",
        "        torch.save(spectrograms_test, os.path.join(spectrogram_directory, \"spec_test\"))\n",
        "        torch.save(murmurs_test, os.path.join(spectrogram_directory, \"murmurs_test\"))\n",
        "        torch.save(outcomes_test, os.path.join(spectrogram_directory, \"outcomes_test\"))\n",
        "    else:\n",
        "        spectrograms_train = torch.load(\n",
        "            os.path.join(spectrogram_directory, \"spec_train\")\n",
        "        )\n",
        "        murmurs_train = torch.load(os.path.join(spectrogram_directory, \"murmurs_train\"))\n",
        "        outcomes_train = torch.load(\n",
        "            os.path.join(spectrogram_directory, \"outcomes_train\")\n",
        "        )\n",
        "        spectrograms_test = torch.load(os.path.join(spectrogram_directory, \"spec_test\"))\n",
        "        murmurs_test = torch.load(os.path.join(spectrogram_directory, \"murmurs_test\"))\n",
        "        outcomes_test = torch.load(os.path.join(spectrogram_directory, \"outcomes_test\"))\n",
        "\n",
        "    return (\n",
        "        spectrograms_train,\n",
        "        murmurs_train,\n",
        "        outcomes_train,\n",
        "        spectrograms_test,\n",
        "        murmurs_test,\n",
        "        outcomes_test,\n",
        "    )\n",
        "\n",
        "\n",
        "def patient_feature_loader(recalc_features, data_directory, output_directory):\n",
        "    if recalc_features == \"True\":\n",
        "        spectrograms, murmurs, outcomes = calc_patient_features(data_directory)\n",
        "        with open(output_directory + \"spectrograms\", \"wb\") as fp:\n",
        "            pickle.dump(spectrograms, fp)\n",
        "        with open(output_directory + \"murmurs\", \"wb\") as fp:\n",
        "            pickle.dump(murmurs, fp)\n",
        "        with open(output_directory + \"outcomes\", \"wb\") as fp:\n",
        "            pickle.dump(outcomes, fp)\n",
        "    else:\n",
        "        with open(output_directory + \"spectrograms\", \"rb\") as fp:\n",
        "            spectrograms = pickle.load(fp)\n",
        "        with open(output_directory + \"murmurs\", \"rb\") as fp:\n",
        "            murmurs = pickle.load(fp)\n",
        "        with open(output_directory + \"outcomes\", \"rb\") as fp:\n",
        "            outcomes = pickle.load(fp)\n",
        "\n",
        "    return spectrograms, murmurs, outcomes\n",
        "\n",
        "\n",
        "# Load recordings.\n",
        "def load_spectrograms(data_directory, data):\n",
        "    num_locations = get_num_locations(data)\n",
        "    recording_information = data.split(\"\\n\")[1 : num_locations + 1]\n",
        "\n",
        "    mel_specs = list()\n",
        "    for i in range(num_locations):\n",
        "        entries = recording_information[i].split(\" \")\n",
        "        recording_file = entries[2]\n",
        "        filename = os.path.join(data_directory, recording_file)\n",
        "        recording, frequency = load_wav_file(filename)\n",
        "        recording = recording / 32768\n",
        "        mel_spec = waveform_to_examples(recording, frequency)\n",
        "        mel_specs.append(mel_spec)\n",
        "    return mel_specs\n",
        "\n",
        "\n",
        "def load_spectrograms_yaseen(file_path):\n",
        "\n",
        "    mel_specs = list()\n",
        "    recording, frequency = load_wav_file(file_path)\n",
        "    recording = recording / 32768\n",
        "    mel_spec = waveform_to_examples(recording, frequency)\n",
        "    mel_specs.append(mel_spec)\n",
        "\n",
        "    return mel_specs\n",
        "\n",
        "\n",
        "def list_wav_files(data_directory):\n",
        "    wav_files = []\n",
        "    subfolder_names = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                wav_files.append(os.path.join(root, file))\n",
        "                subfolder_names.append(os.path.basename(root))\n",
        "\n",
        "    return wav_files, subfolder_names\n",
        "\n",
        "\n",
        "def calc_patient_features(data_directory):\n",
        "\n",
        "    if \"yaseen\" in data_directory:\n",
        "        # Get data and labels\n",
        "        outcome_classes = [f.name for f in os.scandir(data_directory) if f.is_dir()]\n",
        "        murmur_classes = outcome_classes\n",
        "        num_murmur_classes = len(murmur_classes)\n",
        "        num_outcome_classes = len(outcome_classes)\n",
        "        patient_files, labels = list_wav_files(data_directory)\n",
        "        num_patient_files = len(patient_files)\n",
        "        spectrograms = list()\n",
        "        murmurs = list()\n",
        "        outcomes = list()\n",
        "        for label, file_path in zip(labels, patient_files):\n",
        "            # Get labels in the right format\n",
        "            current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
        "            outcome =  label\n",
        "            if outcome in outcome_classes:\n",
        "                j = outcome_classes.index(outcome)\n",
        "                current_outcome[j] = 1\n",
        "            outcomes.append(current_outcome)\n",
        "            murmurs = outcomes\n",
        "            # Spectrograms\n",
        "            current_spectrograms = load_spectrograms_yaseen(file_path)\n",
        "            spectrograms.append(current_spectrograms)\n",
        "\n",
        "    else:\n",
        "        murmur_classes = [\"Present\", \"Unknown\", \"Absent\"]\n",
        "        num_murmur_classes = len(murmur_classes)\n",
        "        outcome_classes = [\"Abnormal\", \"Normal\"]\n",
        "        num_outcome_classes = len(outcome_classes)\n",
        "        patient_files = find_patient_files(data_directory)\n",
        "        num_patient_files = len(patient_files)\n",
        "        spectrograms = list()\n",
        "        murmurs = list()\n",
        "        outcomes = list()\n",
        "        for i in range(num_patient_files):\n",
        "            # Load the current patient data and recordings.\n",
        "            current_patient_data = load_patient_data(patient_files[i])\n",
        "            current_spectrograms = load_spectrograms(data_directory, current_patient_data) # Get spectrograms per patient -> Adjust for Yaseen\n",
        "            spectrograms.append(current_spectrograms)\n",
        "            current_murmur = np.zeros(num_murmur_classes, dtype=int)\n",
        "            murmur = get_murmur(current_patient_data) # -> Adjust for Yaseen\n",
        "            if murmur in murmur_classes:\n",
        "                j = murmur_classes.index(murmur)\n",
        "                current_murmur[j] = 1\n",
        "            murmurs.append(current_murmur)\n",
        "            # Outcome\n",
        "            current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
        "            outcome = get_outcome(current_patient_data) # -> Adjust for Yaseen\n",
        "            if outcome in outcome_classes:\n",
        "                j = outcome_classes.index(outcome)\n",
        "                current_outcome[j] = 1\n",
        "            outcomes.append(current_outcome)\n",
        "\n",
        "    return spectrograms, murmurs, outcomes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "SzUITiWXHlH9",
        "outputId": "bb94fa84-d88c-4f13-905d-c19121503f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'DataProcessing'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-9ba4f05fee23>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from DataProcessing.find_and_load_patient_files import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mfind_patient_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mload_patient_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'DataProcessing'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run torch"
      ],
      "metadata": {
        "id": "For0J5WNH8BX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmKTZgj3H7nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data Splits"
      ],
      "metadata": {
        "id": "g29mNNLat9b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from DataProcessing.find_and_load_patient_files import (\n",
        "#     find_patient_files,\n",
        "#     load_patient_data,\n",
        "# )\n",
        "# from DataProcessing.label_extraction import get_murmur, get_outcome\n",
        "# from DataProcessing.XGBoost_features.metadata import get_metadata\n",
        "\n",
        "\n",
        "def stratified_test_vali_split(\n",
        "    stratified_features: list,\n",
        "    data_directory: str,\n",
        "    stratified_directory: str,\n",
        "    test_size: float,\n",
        "    vali_size: float,\n",
        "    random_states: list = [42],\n",
        "    cv: bool = False,\n",
        "    n_splits: int = 10,\n",
        "    stratified_cv: bool = False,\n",
        "):\n",
        "    # Check if stratified_directory directory exists, otherwise create it.\n",
        "    if not os.path.exists(stratified_directory):\n",
        "        os.makedirs(stratified_directory)\n",
        "\n",
        "    # Get metadata\n",
        "    patient_files = find_patient_files(data_directory)\n",
        "    num_patient_files = len(patient_files)\n",
        "    murmur_classes = [\"Present\", \"Unknown\", \"Absent\"]\n",
        "    num_murmur_classes = len(murmur_classes)\n",
        "    outcome_classes = [\"Abnormal\", \"Normal\"]\n",
        "    num_outcome_classes = len(outcome_classes)\n",
        "    features = list()\n",
        "    murmurs = list()\n",
        "    outcomes = list()\n",
        "    for i in tqdm(range(num_patient_files)):\n",
        "        # Load the current patient data and recordings.\n",
        "        current_patient_data = load_patient_data(patient_files[i])\n",
        "        # Extract features.\n",
        "        current_features = get_metadata(current_patient_data)\n",
        "        current_features = np.insert(\n",
        "            current_features, 0, current_patient_data.split(\" \")[0]\n",
        "        )\n",
        "        current_features = np.insert(\n",
        "            current_features, 1, current_patient_data.split(\" \")[2][:-3]\n",
        "        )\n",
        "        features.append(current_features)\n",
        "        # Extract labels and use one-hot encoding.\n",
        "        # Murmur\n",
        "        current_murmur = np.zeros(num_murmur_classes, dtype=int)\n",
        "        murmur = get_murmur(current_patient_data)\n",
        "        if murmur in murmur_classes:\n",
        "            j = murmur_classes.index(murmur)\n",
        "            current_murmur[j] = 1\n",
        "        murmurs.append(current_murmur)\n",
        "        # Outcome\n",
        "        current_outcome = np.zeros(num_outcome_classes, dtype=int)\n",
        "        outcome = get_outcome(current_patient_data)\n",
        "        if outcome in outcome_classes:\n",
        "            j = outcome_classes.index(outcome)\n",
        "            current_outcome[j] = 1\n",
        "        outcomes.append(current_outcome)\n",
        "    features = np.vstack(features)\n",
        "    murmurs = np.vstack(murmurs)\n",
        "    outcomes = np.vstack(outcomes)\n",
        "\n",
        "    # Combine dataframes\n",
        "    features_pd = pd.DataFrame(\n",
        "        features,\n",
        "        columns=[\n",
        "            \"id\",\n",
        "            \"hz\",\n",
        "            \"age\",\n",
        "            \"female\",\n",
        "            \"male\",\n",
        "            \"height\",\n",
        "            \"weight\",\n",
        "            \"is_pregnant\",\n",
        "        ],\n",
        "    )\n",
        "    murmurs_pd = pd.DataFrame(murmurs, columns=murmur_classes)\n",
        "    outcomes_pd = pd.DataFrame(outcomes, columns=outcome_classes)\n",
        "    complete_pd = pd.concat([features_pd, murmurs_pd, outcomes_pd], axis=1)\n",
        "    complete_pd[\"id\"] = complete_pd[\"id\"].astype(int).astype(str)\n",
        "    complete_pd[\"stratify_column\"] = (\n",
        "        complete_pd[stratified_features].astype(str).agg(\"-\".join, axis=1)\n",
        "    )\n",
        "\n",
        "    # Split data\n",
        "    complete_pd_train_list = list()\n",
        "    complete_pd_val_list = list()\n",
        "    complete_pd_test_list = list()\n",
        "    cnums = list()\n",
        "    if cv:\n",
        "        if stratified_cv:\n",
        "            print(\"Performing stratified cross-validation\")\n",
        "            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "            for i, (train_index, test_index) in enumerate(\n",
        "                skf.split(complete_pd, complete_pd[\"stratify_column\"])\n",
        "            ):\n",
        "                cnums.append(f\"split_{i}\")\n",
        "                complete_pd_train, complete_pd_test = complete_pd.iloc[train_index], complete_pd.iloc[test_index]\n",
        "                vali_split = vali_size / (1 - test_size)\n",
        "                complete_pd_train, complete_pd_val = train_test_split(\n",
        "                    complete_pd_train,\n",
        "                    test_size=vali_split,\n",
        "                    random_state=42,\n",
        "                    stratify=complete_pd_train[\"stratify_column\"],\n",
        "                )\n",
        "                complete_pd_train_list.append(complete_pd_train)\n",
        "                complete_pd_val_list.append(complete_pd_val)\n",
        "                complete_pd_test_list.append(complete_pd_test)\n",
        "        else:\n",
        "            print(\"Performing random cross-validation\")\n",
        "            kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "            for i, (train_index, test_index) in enumerate(\n",
        "                kf.split(complete_pd)\n",
        "            ):\n",
        "                cnums.append(f\"split_{i}\")\n",
        "                complete_pd_train, complete_pd_test = complete_pd.iloc[train_index], complete_pd.iloc[test_index]\n",
        "                vali_split = vali_size / (1 - test_size)\n",
        "                complete_pd_train, complete_pd_val = train_test_split(\n",
        "                    complete_pd_train,\n",
        "                    test_size=vali_split,\n",
        "                    random_state=42,\n",
        "                )\n",
        "                complete_pd_train_list.append(complete_pd_train)\n",
        "                complete_pd_val_list.append(complete_pd_val)\n",
        "                complete_pd_test_list.append(complete_pd_test)\n",
        "    else:\n",
        "        print(\"Performing statified split\")\n",
        "        for random_state in random_states:\n",
        "            cnums.append(f\"seed_{random_state}\")\n",
        "            complete_pd_train, complete_pd_test = train_test_split(\n",
        "                complete_pd,\n",
        "                test_size=test_size,\n",
        "                random_state=random_state,\n",
        "                stratify=complete_pd[\"stratify_column\"],\n",
        "            )\n",
        "            vali_split = vali_size / (1 - test_size)\n",
        "            complete_pd_train, complete_pd_val = train_test_split(\n",
        "                complete_pd_train,\n",
        "                test_size=vali_split,\n",
        "                random_state=random_state + 1,\n",
        "                stratify=complete_pd_train[\"stratify_column\"],\n",
        "            )\n",
        "            complete_pd_train_list.append(complete_pd_train)\n",
        "            complete_pd_val_list.append(complete_pd_val)\n",
        "            complete_pd_test_list.append(complete_pd_test)\n",
        "\n",
        "    # Save the files.\n",
        "    for cnum, complete_pd_train, complete_pd_val, complete_pd_test in zip(\n",
        "        cnums, complete_pd_train_list, complete_pd_val_list, complete_pd_test_list\n",
        "    ):\n",
        "        print(f\"Saving split {cnum} with cv {cv} from {len(cnums)} splits...\")\n",
        "        if cv:\n",
        "            save_folder = os.path.join(stratified_directory, f\"cv_{cv}_stratified_{stratified_cv}\", cnum)\n",
        "        else:\n",
        "            save_folder = os.path.join(stratified_directory, f\"cv_{cv}\", cnum)\n",
        "        os.makedirs(os.path.join(save_folder, \"train_data\"))\n",
        "        os.makedirs(os.path.join(save_folder, \"vali_data\"))\n",
        "        os.makedirs(os.path.join(save_folder, \"test_data\"))\n",
        "        with open(os.path.join(save_folder, \"split_details.txt\"), \"w\") as text_file:\n",
        "            text_file.write(\"This data split is stratified over the following features: \\n\")\n",
        "            for feature in stratified_features:\n",
        "                text_file.write(feature + \", \")\n",
        "        for f in complete_pd_train[\"id\"]:\n",
        "            copy_files(\n",
        "                data_directory,\n",
        "                f,\n",
        "                os.path.join(save_folder, \"train_data/\"),\n",
        "            )\n",
        "        for f in complete_pd_val[\"id\"]:\n",
        "            copy_files(\n",
        "                data_directory,\n",
        "                f,\n",
        "                os.path.join(save_folder, \"vali_data/\"),\n",
        "            )\n",
        "        for f in complete_pd_test[\"id\"]:\n",
        "            copy_files(\n",
        "                data_directory,\n",
        "                f,\n",
        "                os.path.join(save_folder, \"test_data/\"),\n",
        "            )\n",
        "\n",
        "\n",
        "def copy_files(data_directory: str, ident: str, stratified_directory: str) -> None:\n",
        "    # Get the list of files in the data folder.\n",
        "    files = os.listdir(data_directory)\n",
        "    # Copy all files in data_directory that start with f to stratified_directory\n",
        "    for f in files:\n",
        "        if f.startswith(ident):\n",
        "            _ = shutil.copy(os.path.join(data_directory, f), stratified_directory)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ng8G9jDstQN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    print(\"---------------- Starting data_splits.py to split the data ----------------\")\n",
        "\n",
        "    parser = argparse.ArgumentParser(prog=\"StratifiedDataSplit\")\n",
        "    parser.add_argument(\n",
        "        \"--data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory containing the data you wish to split.\",\n",
        "        default=op.join(dir,\"files/circor-heart-sound/1.0.3/training_data\"),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stratified_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory to store the split data.\",\n",
        "        default=dir,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--vali_size\", type=float, default=0.16, help=\"The size of the test split.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_size\", type=float, default=0.2, help=\"The size of the test split.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--cv\", type=bool, default=False, help=\"Whether to run cv.\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--stratified_cv\", type=bool, default=False, help=\"Whether to run cv.\"\n",
        "    )\n",
        "    args = parser.parse_known_args()[0] # Allow unrecognized arguments\n",
        "\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    stratified_features = [\"Normal\", \"Abnormal\", \"Absent\", \"Present\", \"Unknown\"]\n",
        "    print('hello')\n",
        "    # Create the test split.\n",
        "    stratified_test_vali_split(stratified_features, **vars(args))\n",
        "    print('hi')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "mNx88EFnvBYS",
        "outputId": "5325f307-1ae4-42b4-c257-9d0db142fb8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------- Starting data_splits.py to split the data ----------------\n",
            "hello\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 942/942 [03:10<00:00,  4.95it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing statified split\n",
            "Saving split seed_42 with cv False from 1 splits...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileExistsError",
          "evalue": "[Errno 17] File exists: '/content/drive/My Drive/ML/physionet.org/cv_False/seed_42/train_data'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-2ac3efbd3d86>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hello'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Create the test split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mstratified_test_vali_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstratified_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hi'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-2e9212489fdd>\u001b[0m in \u001b[0;36mstratified_test_vali_split\u001b[0;34m(stratified_features, data_directory, stratified_directory, test_size, vali_size, random_states, cv, n_splits, stratified_cv)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0msave_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstratified_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"cv_{cv}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vali_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/content/drive/My Drive/ML/physionet.org/cv_False/seed_42/train_data'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Train ResNet"
      ],
      "metadata": {
        "id": "6yGhYHPeHDv_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Config file containing all of the model hyperparameters.\n",
        "\n",
        "# ResNet parameters\n",
        "dropout = 0.3\n",
        "# Learning rate\n",
        "lr = 3e-5\n",
        "max_overrun = 10\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "pretrained = True\n",
        "# Number of classes for multi class classification\n",
        "n_classes = 3\n",
        "\n",
        "# Parameters for generating the log mel spectrograms used during training.\n",
        "# Architectural constants.\n",
        "NUM_BANDS = 64  # Frequency bands in input mel-spectrogram patch.\n",
        "EMBEDDING_SIZE = 128  # Size of embedding layer.\n",
        "# Hyperparameters used in feature and example generation.\n",
        "SAMPLE_RATE = 4000 # 8000 for Yaseen, else 4000\n",
        "STFT_WINDOW_LENGTH_SECONDS = 0.025\n",
        "STFT_HOP_LENGTH_SECONDS = 0.010\n",
        "NUM_MEL_BINS = NUM_BANDS\n",
        "MEL_MIN_HZ = 10\n",
        "MEL_MAX_HZ = 2000\n",
        "LOG_OFFSET = 0.01  # Offset used for stabilized log of input mel-spectrogram.\n",
        "EXAMPLE_WINDOW_SECONDS = 4.0 # 0.5 for Yaseen, else 4.0\n",
        "EXAMPLE_HOP_SECONDS = 1.0 # 0.01 for Yaseen, else 1.0"
      ],
      "metadata": {
        "id": "jxU_KSqcHX7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "from Config import hyperparameters\n",
        "from HumBugDB.ResNetDropoutSource import resnet50dropout\n",
        "from HumBugDB.ResNetSource import resnet50\n",
        "\n",
        "\n",
        "class ResnetFull(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResnetFull, self).__init__()\n",
        "        self.resnet = resnet50(pretrained=hyperparameters.pretrained)\n",
        "        self.n_channels = 3\n",
        "        # Remove final linear layer\n",
        "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        self.fc1 = nn.Linear(2048, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x).squeeze()\n",
        "        x = self.fc1(x)\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetDropoutFull(nn.Module):\n",
        "    def __init__(self, dropout=0.2, bayesian=True):\n",
        "        super(ResnetDropoutFull, self).__init__()\n",
        "        self.dropout = dropout\n",
        "        self.resnet = resnet50dropout(\n",
        "            pretrained=hyperparameters.pretrained, dropout_p=self.dropout, bayesian=bayesian\n",
        "        )\n",
        "        self.n_channels = 3\n",
        "        # Remove final linear layer\n",
        "        self.resnet = nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
        "        self.fc1 = nn.Linear(2048, 1)\n",
        "        self.bayesian = bayesian\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.bayesian == True:\n",
        "            training = True\n",
        "        else:\n",
        "            training = self.training\n",
        "        x = self.resnet(x).squeeze()\n",
        "        x = self.fc1(F.dropout(x, p=self.dropout, training=training))\n",
        "        x = torch.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def make_weights_for_balanced_classes(images, nclasses):\n",
        "    count = [0] * nclasses\n",
        "    for item in images:\n",
        "        count[torch.argmax(item[1])] += 1\n",
        "    weight_per_class = [0.0] * nclasses\n",
        "    N = float(sum(count))\n",
        "    for i in range(nclasses):\n",
        "        weight_per_class[i] = N / float(count[i])\n",
        "    weight = [0] * len(images)\n",
        "    for idx, val in enumerate(images):\n",
        "        weight[idx] = weight_per_class[torch.argmax(val[1])]\n",
        "    return weight\n",
        "\n",
        "\n",
        "def build_dataloader(\n",
        "    x_train, y_train, x_val=None, y_val=None, shuffle=True, sampler=None\n",
        "):\n",
        "    x_train = x_train.clone().detach().float() #torch.tensor(x_train).float()\n",
        "    y_train = y_train.clone().detach().float() #torch.tensor(y_train).float()\n",
        "    train_dataset = TensorDataset(x_train, y_train)\n",
        "    if sampler is None:\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=hyperparameters.batch_size, shuffle=shuffle\n",
        "        )\n",
        "    else:\n",
        "        weights = make_weights_for_balanced_classes(train_dataset, 2)\n",
        "        weights = torch.DoubleTensor(weights)\n",
        "        sampler = torch.utils.data.sampler.WeightedRandomSampler(weights, len(weights))\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset, batch_size=hyperparameters.batch_size, sampler=sampler\n",
        "        )\n",
        "\n",
        "    if x_val is not None:\n",
        "        x_val = x_val.clone().detach().float() #torch.tensor(x_val).float()\n",
        "        y_val = y_val.clone().detach().float() #torch.tensor(y_val).float()\n",
        "        val_dataset = TensorDataset(x_val, y_val)\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset, batch_size=hyperparameters.batch_size, shuffle=shuffle\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader\n",
        "    return train_loader\n",
        "\n",
        "\n",
        "def train_model(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    clas_weight=None,\n",
        "    x_val=None,\n",
        "    y_val=None,\n",
        "    model=ResnetDropoutFull(),\n",
        "    model_name=\"test\",\n",
        "    model_dir=\"models\",\n",
        "    sampler=None,\n",
        "):\n",
        "\n",
        "    if not os.path.isdir(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    if x_val is not None:\n",
        "        train_loader, val_loader = build_dataloader(\n",
        "            x_train, y_train, x_val, y_val, sampler=sampler\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        train_loader = build_dataloader(x_train, y_train, sampler=sampler)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Using data parallel\")\n",
        "        model = nn.DataParallel(\n",
        "            model, device_ids=list(range(torch.cuda.device_count()))\n",
        "        )\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimiser = optim.Adam(model.parameters(), lr=hyperparameters.lr)\n",
        "\n",
        "    all_train_loss = []\n",
        "    all_train_metric = []\n",
        "    all_val_loss = []\n",
        "    all_val_metric = []\n",
        "    best_val_acc = -np.inf\n",
        "\n",
        "    best_train_acc = -np.inf\n",
        "    e_saved = None\n",
        "    output_string_to_save = \"\"\n",
        "    overrun_counter = 0\n",
        "    for e in range(hyperparameters.epochs):\n",
        "        start_time = time.time()\n",
        "        train_loss = 0.0\n",
        "        model.train()\n",
        "\n",
        "        all_y = []\n",
        "        all_y_pred = []\n",
        "        for batch_i, inputs in enumerate(train_loader):\n",
        "\n",
        "            x = inputs[:-1][0].repeat(1, 3, 1, 1)\n",
        "            y = torch.argmax(inputs[1], dim=1, keepdim=True).float()\n",
        "\n",
        "            optimiser.zero_grad()\n",
        "            y_pred = model(x)\n",
        "            if clas_weight is not None:\n",
        "                criterion.weight = (clas_weight[1] - clas_weight[0]) * y + clas_weight[\n",
        "                    0\n",
        "                ]\n",
        "                loss = criterion.forward(y_pred, y)\n",
        "            else:\n",
        "                loss = criterion(y_pred, y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            all_y.append(y.cpu().detach())\n",
        "            all_y_pred.append(y_pred.cpu().detach())\n",
        "\n",
        "            del x\n",
        "            del y\n",
        "\n",
        "        all_train_loss.append(train_loss / len(train_loader))\n",
        "\n",
        "        all_y = torch.cat(all_y)\n",
        "        all_y_pred = torch.cat(all_y_pred)\n",
        "        train_metric = balanced_accuracy_score(\n",
        "            all_y.numpy(), (all_y_pred.numpy() > 0.5).astype(float)\n",
        "        )\n",
        "        all_train_metric.append(train_metric)\n",
        "\n",
        "        if x_val is not None:\n",
        "            val_loss, val_metric = test_model(\n",
        "                model, val_loader, clas_weight, criterion, device=device\n",
        "            )\n",
        "            all_val_loss.append(val_loss)\n",
        "            all_val_metric.append(val_metric)\n",
        "\n",
        "            acc_metric = val_metric\n",
        "            best_acc_metric = best_val_acc\n",
        "        else:\n",
        "            acc_metric = train_metric\n",
        "            best_acc_metric = best_train_acc\n",
        "        if acc_metric > best_acc_metric:\n",
        "\n",
        "            checkpoint_name = f\"model_{model_name}.pth\"\n",
        "            e_saved = e\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(model_dir, checkpoint_name),\n",
        "            )\n",
        "            print(\n",
        "                \"Saving model to:\",\n",
        "                os.path.join(model_dir, checkpoint_name),\n",
        "            )\n",
        "            best_train_acc = train_metric\n",
        "            if x_val is not None:\n",
        "                best_val_acc = val_metric\n",
        "            overrun_counter = -1\n",
        "\n",
        "        overrun_counter += 1\n",
        "        if x_val is not None:\n",
        "            output_string = (\n",
        "                \"Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, Val Loss: %.8f, \"\n",
        "                \"Val Acc: %.8f, overrun_counter %i\"\n",
        "                % (\n",
        "                    e,\n",
        "                    train_loss / len(train_loader),\n",
        "                    train_metric,\n",
        "                    val_loss,\n",
        "                    val_metric,\n",
        "                    overrun_counter,\n",
        "                )\n",
        "            )\n",
        "        else:\n",
        "            output_string = (\n",
        "                \"Epoch: %d, Train Loss: %.8f, Train Acc: %.8f, overrun_counter %i\"\n",
        "                % (e, train_loss / len(train_loader), train_metric, overrun_counter)\n",
        "            )\n",
        "        print(output_string)\n",
        "        output_string_to_save += output_string + \"\\n\"\n",
        "        print(f\"Training epoch {e} took {round((time.time()-start_time)/60,4)} min.\")\n",
        "        if overrun_counter > hyperparameters.max_overrun:\n",
        "            break\n",
        "\n",
        "    if e_saved is not None:\n",
        "        checkpoint_name = f\"model_{model_name}_final.pth\"\n",
        "        torch.save(\n",
        "            model.state_dict(),\n",
        "            os.path.join(model_dir, checkpoint_name),\n",
        "        )\n",
        "        print(\n",
        "            \"Saving model to:\",\n",
        "            os.path.join(model_dir, checkpoint_name),\n",
        "        )\n",
        "\n",
        "    # Save output string\n",
        "    output_string_to_save += f\"Best epoch: {e_saved}\\n\"\n",
        "    with open(os.path.join(model_dir, f\"output_{model_name}.txt\"), \"w\") as f:\n",
        "        f.write(output_string_to_save)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_model(model, test_loader, clas_weight, criterion, device=None):\n",
        "    with torch.no_grad():\n",
        "        if device is None:\n",
        "            torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "        test_loss = 0.0\n",
        "        model.eval()\n",
        "\n",
        "        all_y = []\n",
        "        all_y_pred = []\n",
        "        counter = 1\n",
        "        for inputs in test_loader:\n",
        "\n",
        "            x = inputs[:-1][0].repeat(1, 3, 1, 1)\n",
        "            y = torch.argmax(inputs[1], dim=1, keepdim=True).float()\n",
        "\n",
        "            if len(x) == 1:\n",
        "                x = x[0]\n",
        "\n",
        "            y_pred = model(x)\n",
        "\n",
        "            if clas_weight is not None:\n",
        "                criterion.weight = (clas_weight[1] - clas_weight[0]) * y + clas_weight[\n",
        "                    0\n",
        "                ]\n",
        "                loss = criterion.forward(y_pred, y)\n",
        "            else:\n",
        "                loss = criterion(y_pred, y)\n",
        "            test_loss += loss.item()\n",
        "            all_y.append(y.cpu().detach())\n",
        "            all_y_pred.append(y_pred.cpu().detach())\n",
        "\n",
        "            del x\n",
        "            del y\n",
        "            del y_pred\n",
        "\n",
        "            counter += 1\n",
        "\n",
        "        all_y = torch.cat(all_y)\n",
        "        all_y_pred = torch.cat(all_y_pred)\n",
        "        test_metric = balanced_accuracy_score(\n",
        "            all_y.numpy(), (all_y_pred.numpy() > 0.5).astype(float)\n",
        "        )\n",
        "        test_loss = test_loss / len(test_loader)\n",
        "\n",
        "    return test_loss, test_metric\n",
        "\n",
        "\n",
        "def load_model(filepath, model=ResnetDropoutFull()):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "\n",
        "    if torch.cuda.device_count() > 1:\n",
        "        print(\"Using data parallel\")\n",
        "        model = nn.DataParallel(\n",
        "            model, device_ids=list(range(torch.cuda.device_count()))\n",
        "        )\n",
        "    model = model.to(device)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        map_location = lambda storage, loc: storage.cuda()\n",
        "    elif torch.backends.mps.is_available():\n",
        "        map_location = lambda storage, loc: storage.mps()\n",
        "    else:\n",
        "        map_location = torch.device(\"cpu\")\n",
        "    model.load_state_dict(torch.load(filepath, map_location=map_location))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "G-YK2k8zL2ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "\n",
        "# from Config import hyperparameters\n",
        "# from DataProcessing.net_feature_extractor import net_feature_loader\n",
        "from HumBugDB.runTorch import ResnetDropoutFull as ResnetDropoutBinary\n",
        "from HumBugDB.runTorch import ResnetFull as ResnetBinary\n",
        "from HumBugDB.runTorch import train_model as train_model_binary\n",
        "from HumBugDB.runTorchMultiClass import ResnetDropoutFull as ResnetDropoutMulti\n",
        "from HumBugDB.runTorchMultiClass import ResnetFull as ResnetMulti\n",
        "from HumBugDB.runTorchMultiClass import train_model as train_model_multi\n",
        "\n",
        "\n",
        "def create_model(model_name, num_classes, bayesian):\n",
        "    if model_name == \"resnet50\":\n",
        "        print(\"Running resnet without dropout\")\n",
        "        if num_classes == 2:\n",
        "            model = ResnetBinary()\n",
        "            training = train_model_binary\n",
        "        else:\n",
        "            model = ResnetMulti(num_classes)\n",
        "            training = train_model_multi\n",
        "    elif model_name == \"resnet50dropout\":\n",
        "        print(f\"Creating dropout model with bayesian: {bayesian}\")\n",
        "        if num_classes == 2:\n",
        "            model = ResnetDropoutBinary(dropout=hyperparameters.dropout, bayesian=bayesian)\n",
        "            training = train_model_binary\n",
        "        else:\n",
        "            model = ResnetDropoutMulti(n_classes=num_classes, bayesian=bayesian)\n",
        "            training = train_model_multi\n",
        "    else:\n",
        "        raise NotImplementedError(\"Only implemented resnet50 and resnet50dropout\")\n",
        "\n",
        "    return model, training\n",
        "\n",
        "\n",
        "def run_model_training(\n",
        "    recalc_features,\n",
        "    train_data_directory,\n",
        "    vali_data_directory,\n",
        "    spectrogram_directory,\n",
        "    model_name,\n",
        "    model_label,\n",
        "    model_dir,\n",
        "    classes_name,\n",
        "    bayesian,\n",
        "    weights,\n",
        "):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    (\n",
        "        spectrograms_train,\n",
        "        murmurs_train,\n",
        "        outcomes_train,\n",
        "        spectrograms_test,\n",
        "        murmurs_test,\n",
        "        outcomes_test,\n",
        "    ) = net_feature_loader(\n",
        "        recalc_features,\n",
        "        train_data_directory,\n",
        "        vali_data_directory,\n",
        "        spectrogram_directory,\n",
        "    )\n",
        "    print(\"Data loaded\")\n",
        "\n",
        "    X_train = spectrograms_train.to(device)\n",
        "    X_test = spectrograms_test.to(device)\n",
        "    if classes_name == \"murmur\":\n",
        "        y_train = murmurs_train.to(device)\n",
        "        y_test = murmurs_test.to(device)\n",
        "        model, training = create_model(model_name, 3, bayesian)\n",
        "        training(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            clas_weight=weights,\n",
        "            x_val=X_test,\n",
        "            y_val=y_test,\n",
        "            model=model,\n",
        "            model_name=model_label,\n",
        "            model_dir=model_dir,\n",
        "        )\n",
        "    elif classes_name == \"outcome_binary\":\n",
        "        y_train = outcomes_train.to(device)\n",
        "        y_test = outcomes_test.to(device)\n",
        "        model, training = create_model(model_name, 2, bayesian)\n",
        "        training(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            clas_weight=weights,\n",
        "            x_val=X_test,\n",
        "            y_val=y_test,\n",
        "            model=model,\n",
        "            model_name=model_label,\n",
        "            model_dir=model_dir,\n",
        "        )\n",
        "    elif classes_name == \"murmur_binary\":\n",
        "        knowledge_train = torch.zeros((murmurs_train.shape[0], 2))\n",
        "        for i in range(len(murmurs_train)):\n",
        "            if (\n",
        "                torch.argmax(murmurs_train[i]) == 0\n",
        "                or torch.argmax(murmurs_train[i]) == 1\n",
        "            ):\n",
        "                knowledge_train[i, 0] = 1\n",
        "            else:\n",
        "                knowledge_train[i, 1] = 1\n",
        "        knowledge_test = torch.zeros((murmurs_test.shape[0], 2))\n",
        "        for i in range(len(murmurs_test)):\n",
        "            if torch.argmax(murmurs_test[i]) == 0 or torch.argmax(murmurs_test[i]) == 1:\n",
        "                knowledge_test[i, 0] = 1\n",
        "            else:\n",
        "                knowledge_test[i, 1] = 1\n",
        "        y_train = knowledge_train.to(device)\n",
        "        y_test = knowledge_test.to(device)\n",
        "        model, training = create_model(model_name, 2, bayesian)\n",
        "        training(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            clas_weight=weights,\n",
        "            x_val=X_test,\n",
        "            y_val=y_test,\n",
        "            model=model,\n",
        "            model_name=model_label,\n",
        "            model_dir=model_dir,\n",
        "        )\n",
        "    elif classes_name == \"binary_present\":\n",
        "        knowledge_train = torch.zeros((murmurs_train.shape[0], 2))\n",
        "        for i in range(len(murmurs_train)):\n",
        "            if (\n",
        "                torch.argmax(murmurs_train[i]) == 1\n",
        "                or torch.argmax(murmurs_train[i]) == 2\n",
        "            ):\n",
        "                knowledge_train[i, 1] = 1\n",
        "            else:\n",
        "                knowledge_train[i, 0] = 1\n",
        "        knowledge_test = torch.zeros((murmurs_test.shape[0], 2))\n",
        "        for i in range(len(murmurs_test)):\n",
        "            if torch.argmax(murmurs_test[i]) == 1 or torch.argmax(murmurs_test[i]) == 2:\n",
        "                knowledge_test[i, 1] = 1\n",
        "            else:\n",
        "                knowledge_test[i, 0] = 1\n",
        "        y_train = knowledge_train.to(device)\n",
        "        y_test = knowledge_test.to(device)\n",
        "        model, training = create_model(model_name, 2, bayesian)\n",
        "        training(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            clas_weight=weights,\n",
        "            x_val=X_test,\n",
        "            y_val=y_test,\n",
        "            model=model,\n",
        "            model_name=model_label,\n",
        "            model_dir=model_dir,\n",
        "        )\n",
        "    elif classes_name == \"binary_unknown\":\n",
        "        knowledge_train = torch.zeros((murmurs_train.shape[0], 2))\n",
        "        for i in range(len(murmurs_train)):\n",
        "            if (\n",
        "                torch.argmax(murmurs_train[i]) == 0\n",
        "                or torch.argmax(murmurs_train[i]) == 2\n",
        "            ):\n",
        "                knowledge_train[i, 1] = 1\n",
        "            else:\n",
        "                knowledge_train[i, 0] = 1\n",
        "        knowledge_test = torch.zeros((murmurs_test.shape[0], 2))\n",
        "        for i in range(len(murmurs_test)):\n",
        "            if torch.argmax(murmurs_test[i]) == 0 or torch.argmax(murmurs_test[i]) == 2:\n",
        "                knowledge_test[i, 1] = 1\n",
        "            else:\n",
        "                knowledge_test[i, 0] = 1\n",
        "        y_train = knowledge_train.to(device)\n",
        "        y_test = knowledge_test.to(device)\n",
        "        model, training = create_model(model_name, 2, bayesian)\n",
        "        training(\n",
        "            X_train,\n",
        "            y_train,\n",
        "            clas_weight=weights,\n",
        "            x_val=X_test,\n",
        "            y_val=y_test,\n",
        "            model=model,\n",
        "            model_name=model_label,\n",
        "            model_dir=model_dir,\n",
        "            sampler=True,\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\"classes_name must be one of outcome, murmur or knowledge.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(prog=\"TrainResNet\")\n",
        "    parser.add_argument(\n",
        "        \"--recalc_features\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether or not to recalculate the log mel spectrograms used as \"\n",
        "        \"input to the ResNet.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-recalc_features\", dest=\"recalc_features\", action=\"store_false\"\n",
        "    )\n",
        "    parser.set_defaults(recalc_features=True)\n",
        "    parser.add_argument(\n",
        "        \"--train_data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory of the training data.\",\n",
        "        default=\"data/stratified_data/train_data\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--vali_data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory of the validation data.\",\n",
        "        default=\"data/stratified_data/vali_data\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--spectrogram_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory in which to save the spectrogram training data.\",\n",
        "        default=\"data/spectrograms\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        help=\"The ResNet to train. Current options are resnet50 or resnet50dropout.\",\n",
        "        choices=[\"resnet50\", \"resnet50dropout\"],\n",
        "        default=\"resnet50dropout\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_label\",\n",
        "        type=str,\n",
        "        help=\"The label to use when saving the model.\",\n",
        "        default=\"ResNetDropout\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_dir\",\n",
        "        type=str,\n",
        "        help=\"The directory to use when saving the model.\",\n",
        "        default=\"data/models\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--classes_name\",\n",
        "        type=str,\n",
        "        help=\"The name of the classes to train the model on.\",\n",
        "        choices=[\"murmur\", \"outcome_binary\", \"murmur_binary\", \"binary_present\", \"binary_unknown\"],\n",
        "        default=\"murmur\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--disable-bayesian',\n",
        "        dest='bayesian',\n",
        "        action='store_false',\n",
        "        default=True,\n",
        "        help='Disable Bayesian features (default: Bayesian is enabled)'\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--weights_str\",\n",
        "        type=str,\n",
        "        help=\"String containing the class weights for a weighted loss function, \"\n",
        "        \"e.g.5,3,1.\",\n",
        "        default=None,\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    weights = None\n",
        "    if args.weights_str:\n",
        "        weights = [int(x) for x in args.weights_str.split(\",\")]\n",
        "    vars(args).popitem()\n",
        "\n",
        "    print(\"---------------- Starting train_resnet.py for training ----------------\")\n",
        "    print(f\"---------------- Using data from {args.train_data_directory}\")\n",
        "\n",
        "    run_model_training(**vars(args), weights=weights)"
      ],
      "metadata": {
        "id": "3e3j68VOHHXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: dbres"
      ],
      "metadata": {
        "id": "qmNKNQw7Jaek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from DataProcessing.find_and_load_patient_files import (\n",
        "    find_patient_files,\n",
        "    load_patient_data,\n",
        ")\n",
        "from DataProcessing.helper_code import get_num_locations, load_recordings, load_wav_file\n",
        "from DataProcessing.net_feature_extractor import load_spectrograms_yaseen, load_spectrograms_yaseen\n",
        "from HumBugDB.LogMelSpecs.compute_LogMelSpecs import waveform_to_examples\n",
        "from HumBugDB.runTorch import load_model\n",
        "from ModelEvaluation.evaluate_model import evaluate_model\n",
        "from train_resnet import create_model\n",
        "\n",
        "from Config import hyperparameters\n",
        "\n",
        "\n",
        "def list_wav_files(data_directory):\n",
        "    wav_files = []\n",
        "    subfolder_names = []\n",
        "\n",
        "    for root, dirs, files in os.walk(data_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                wav_files.append(os.path.join(root, file))\n",
        "                subfolder_names.append(os.path.basename(root))\n",
        "\n",
        "    return wav_files, subfolder_names\n",
        "\n",
        "\n",
        "def get_binary_spectrogram_outputs(\n",
        "    spectrograms,\n",
        "    model_binary_present,\n",
        "    model_binary_unknown,\n",
        "    model_binary\n",
        "):\n",
        "    if (model_binary_present is not None) and (model_binary_unknown is not None):\n",
        "        model_outputs_unknown = []\n",
        "        model_outputs_present = []\n",
        "        unknown_probabilities = []\n",
        "        present_probabilities = []\n",
        "        for spectrogram in spectrograms:\n",
        "            output_present = (\n",
        "                calc_patient_output(model_binary_present, [spectrogram], repeats=30)\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            output_unknown = (\n",
        "                calc_patient_output(model_binary_unknown, [spectrogram], repeats=30)\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            model_outputs_present.append(output_present)\n",
        "            model_outputs_unknown.append(output_unknown)\n",
        "            present_probabilities.append(\n",
        "                np.array([1 - output_present[0], output_present[0]])\n",
        "            )\n",
        "            unknown_probabilities.append(\n",
        "                np.array([1 - output_unknown[0], output_unknown[0]])\n",
        "            )\n",
        "        present_probability = np.mean(np.array(present_probabilities), axis=0)\n",
        "        unknown_probability = np.mean(np.array(unknown_probabilities), axis=0)\n",
        "        outputs = []\n",
        "        idx_unknown = (np.mean(np.array(model_outputs_unknown)) > 0.5).astype(float)\n",
        "        idx_present = (np.mean(np.array(model_outputs_present)) > 0.5).astype(float)\n",
        "        if idx_present == 0:\n",
        "            outputs.append(np.array([1, 0, 0]))\n",
        "        elif idx_unknown == 0:\n",
        "            outputs.append(np.array([0, 1, 0]))\n",
        "        else:\n",
        "            outputs.append(np.array([0, 0, 1]))\n",
        "\n",
        "        probabilities = [\n",
        "            present_probability[0],\n",
        "            present_probability[1] * unknown_probability[0],\n",
        "            present_probability[1] * unknown_probability[1],\n",
        "        ]\n",
        "    elif model_binary is not None:\n",
        "        model_outputs = []\n",
        "        probabilities = []\n",
        "        for spectrogram in spectrograms:\n",
        "            output = (\n",
        "                calc_patient_output(model_binary, [spectrogram], repeats=30)\n",
        "                .cpu()\n",
        "                .numpy()\n",
        "            )\n",
        "            model_outputs.append(output)\n",
        "            probabilities.append(np.array([1 - output[0], output[0]]))\n",
        "        probability = np.mean(np.array(probabilities), axis=0)\n",
        "        outputs = []\n",
        "        idx = (np.mean(np.array(model_outputs)) > 0.5).astype(float)\n",
        "        if idx == 0:\n",
        "            outputs.append(np.array([1, 0]))\n",
        "        else:\n",
        "            outputs.append(np.array([0, 1]))\n",
        "        probabilities = [probability[0], probability[1]]\n",
        "\n",
        "    return outputs[0].tolist(), probabilities\n",
        "\n",
        "\n",
        "def calc_patient_output(model, recording_spectrograms, repeats):\n",
        "    model.eval()\n",
        "    outputs = []\n",
        "    for location in recording_spectrograms:\n",
        "        input = location.repeat(1, 3, 1, 1)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        input = input.to(device)\n",
        "        model_out = []\n",
        "        for _ in range(repeats):\n",
        "            out = model(input)\n",
        "            out = out.cpu().detach().unsqueeze(2)\n",
        "            model_out.append(out)\n",
        "        model_out = torch.mean(torch.cat(model_out, dim=2), dim=2)\n",
        "        outputs.append(torch.mean(model_out, axis=0).unsqueeze(dim=0))\n",
        "    output = torch.mean(torch.cat(outputs), axis=0).detach()\n",
        "    return output\n",
        "\n",
        "\n",
        "def calculate_dbres_output(\n",
        "    model_name,\n",
        "    recalc_output,\n",
        "    data_directory,\n",
        "    output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    recordings_file: str = \"\",\n",
        "    bayesian: bool = True\n",
        "):\n",
        "\n",
        "    if recalc_output:\n",
        "\n",
        "        if not os.path.exists(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "\n",
        "        # Get model\n",
        "        model_binary_present = create_model(model_name, 2, bayesian)\n",
        "        model_binary_unknown = create_model(model_name, 2, bayesian)\n",
        "        model_binary = create_model(model_name, 2, bayesian)\n",
        "\n",
        "        # Load model\n",
        "        if (model_binary_present_pth is not None) and (model_binary_unknown_pth is not None):\n",
        "            print(\"Loading multiclass model\")\n",
        "            model_binary_present = load_model(\n",
        "                model_binary_present_pth, model=model_binary_present[0]\n",
        "            )\n",
        "            model_binary_unknown = load_model(\n",
        "                model_binary_unknown_pth, model=model_binary_unknown[0]\n",
        "            )\n",
        "            model_binary = None\n",
        "        elif model_binary_pth is not None:\n",
        "            print(\"Loading binary model\")\n",
        "            model_binary = load_model(model_binary_pth, model=model_binary[0])\n",
        "            model_binary_present = None\n",
        "            model_binary_unknown = None\n",
        "        else:\n",
        "            raise Exception(\"No model was provided.\")\n",
        "\n",
        "        # Get data\n",
        "        murmur_probabilities = list()\n",
        "        murmur_outputs = list()\n",
        "        labels = None\n",
        "        if len(recordings_file) > 0:\n",
        "            patient_files = pd.read_csv(recordings_file)\n",
        "        else:\n",
        "            if \"yaseen\" in data_directory:\n",
        "                outcome_classes = [f.name for f in os.scandir(data_directory) if f.is_dir()]\n",
        "                murmur_classes = outcome_classes\n",
        "                patient_files, labels = list_wav_files(data_directory)\n",
        "            else:\n",
        "                patient_files = find_patient_files(data_directory)\n",
        "\n",
        "        # Get count of patient files\n",
        "        num_patient_files = len(patient_files)\n",
        "        if num_patient_files == 0:\n",
        "            print(f\"No data was provided in {data_directory} for recordings_file {recordings_file}.\")\n",
        "            raise Exception(\"No data was provided.\")\n",
        "\n",
        "        # Get spectrograms and predictions\n",
        "        for i in tqdm(range(num_patient_files)):\n",
        "            if len(recordings_file) > 0:\n",
        "                current_patient_data = patient_files.iloc[i]\n",
        "                current_recordings = list()\n",
        "                recording, frequency = load_wav_file(patient_files[\"path\"].iloc[i])\n",
        "                current_recordings.append(recording)\n",
        "                sample_rate = frequency\n",
        "                num_locations = 1\n",
        "            else:\n",
        "                if \"yaseen\" in data_directory:\n",
        "                    pass\n",
        "                else:\n",
        "                    sample_rate=hyperparameters.SAMPLE_RATE\n",
        "                    current_patient_data = load_patient_data(patient_files[i])\n",
        "                    current_recordings = load_recordings(data_directory, current_patient_data)\n",
        "                    num_locations = get_num_locations(current_patient_data)\n",
        "\n",
        "            # Get spectrograms\n",
        "            if \"yaseen\" in data_directory:\n",
        "                spectrograms = load_spectrograms_yaseen(patient_files[i])\n",
        "            else:\n",
        "                current_recordings = [r / 32768 for r in current_recordings]\n",
        "                spectrograms = list()\n",
        "                for j in range(num_locations):\n",
        "                    mel_spec = waveform_to_examples(\n",
        "                        data=current_recordings[j], sample_rate=sample_rate\n",
        "                    )\n",
        "                    spectrograms.append(mel_spec)\n",
        "\n",
        "            # Get predictions\n",
        "            murmur_output, murmur_probability = get_binary_spectrogram_outputs(\n",
        "                spectrograms, model_binary_present, model_binary_unknown, model_binary\n",
        "            )\n",
        "            murmur_probabilities.append(murmur_probability)\n",
        "            murmur_outputs.append(murmur_output)\n",
        "\n",
        "        # Store\n",
        "        murmur_probabilities = np.vstack(murmur_probabilities)\n",
        "        np.save(\n",
        "            os.path.join(output_directory, \"probabilities.npy\"),\n",
        "            murmur_probabilities,\n",
        "        )\n",
        "        murmur_outputs = np.vstack(murmur_outputs)\n",
        "        np.save(os.path.join(output_directory, \"outputs.npy\"), murmur_outputs)\n",
        "    else:\n",
        "        murmur_probabilities = np.load(\n",
        "            os.path.join(output_directory, \"probabilities.npy\")\n",
        "        )\n",
        "        murmur_outputs = np.load(os.path.join(output_directory, \"outputs.npy\"))\n",
        "\n",
        "    return murmur_probabilities, murmur_outputs, labels\n",
        "\n",
        "\n",
        "def calculate_dbres_scores(\n",
        "    model_name,\n",
        "    recalc_output,\n",
        "    data_directory,\n",
        "    output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    recordings_file: str = \"\",\n",
        "    bayesian: bool = True\n",
        "):\n",
        "\n",
        "    probabilities, outputs, labels = calculate_dbres_output(\n",
        "        model_name,\n",
        "        recalc_output,\n",
        "        data_directory,\n",
        "        output_directory,\n",
        "        model_binary_pth,\n",
        "        model_binary_present_pth,\n",
        "        model_binary_unknown_pth,\n",
        "        recordings_file,\n",
        "        bayesian\n",
        "    )\n",
        "\n",
        "    if (model_binary_present_pth is not None) and (model_binary_unknown_pth is not None):\n",
        "        model_type = \"murmur\"\n",
        "    elif model_binary_pth is not None:\n",
        "        if (\"MurmurBinary\" in model_binary_pth) or (\"Murmur_Binary\" in model_binary_pth):\n",
        "            model_type = \"murmur_binary\"\n",
        "        elif (\"OutcomeBinary\" in model_binary_pth) or (\"Outcome_Binary\" in model_binary_pth):\n",
        "            model_type = \"outcome_binary\"\n",
        "        else:\n",
        "            raise Exception(\"No binary murmur or outcome model was provided.\")\n",
        "    else:\n",
        "        raise Exception(\"No model was provided.\")\n",
        "\n",
        "    print(f\"--- Evaluating {model_type} model ---\")\n",
        "    if \"yaseen\" in data_directory:\n",
        "        scores = evaluate_model(data_directory, probabilities, outputs, model_type, recordings_file = recordings_file, output_directory = output_directory, true_labels = labels)\n",
        "    else:\n",
        "        scores = evaluate_model(data_directory, probabilities, outputs, model_type, recordings_file = recordings_file, output_directory = output_directory)\n",
        "\n",
        "    print(\"--- DBRes scores ---\")\n",
        "    print(f\"{scores}\")\n",
        "    with open(os.path.join(output_directory, \"DBRes_score.npy\"), \"w\") as text_file:\n",
        "        text_file.write(scores)\n",
        "\n",
        "    if model_type == \"murmur\":\n",
        "        print(f\"--- Evaluating {model_type} model as binary ---\")\n",
        "        # Combine element at position 0 and 1 to get binary output, but keep position 2\n",
        "        outputs_binary = np.vstack(\n",
        "            [np.logical_or(outputs[:, 0], outputs[:, 1]), outputs[:, 2]]\n",
        "        ).T\n",
        "        probabilities_binary = np.vstack(\n",
        "            [np.max(probabilities[:, :2], axis=1), probabilities[:, 2]]\n",
        "        ).T\n",
        "        scores_binary = evaluate_model(\n",
        "            data_directory, probabilities_binary, outputs_binary, \"murmur_binary\", recordings_file = recordings_file, output_directory = output_directory\n",
        "        )\n",
        "        print(\"--- DBRes scores binary ---\")\n",
        "        print(f\"{scores_binary}\")\n",
        "        with open(os.path.join(output_directory, \"DBRes_score_binary.npy\"), \"w\") as text_file:\n",
        "            text_file.write(scores_binary)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(prog=\"DBRes\")\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        help=\"The ResNet to train. Current options are resnet50 or resnet50dropout.\",\n",
        "        choices=[\"resnet50\", \"resnet50dropout\"],\n",
        "        default=\"resnet50dropout\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--recalc_output\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether or not to recalculate the output from DBRes.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--no-recalc_output\", dest=\"recalc_output\", action=\"store_false\"\n",
        "    )\n",
        "    parser.set_defaults(recalc_output=True)\n",
        "    parser.add_argument(\n",
        "        \"--data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory of the data.\",\n",
        "        default=\"data/stratified_data/test_data\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory in which to save DBRes's output.\",\n",
        "        default=\"data/dbres_outputs\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify present vs not present.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_present_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify present vs not present.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_unknown_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify unknown vs not unknown.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--recordings_file\",\n",
        "        type=str,\n",
        "        help=\"The path to a recordings file.\",\n",
        "        default=\"\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--disable-bayesian',\n",
        "        dest='bayesian',\n",
        "        action='store_false',\n",
        "        default=True,\n",
        "        help='Disable Bayesian features (default: Bayesian is enabled)'\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"---------------- Starting dbres.py for predictions and evaluations ----------------\")\n",
        "    if len(args.recordings_file) > 0:\n",
        "        print(f\"---------------- Using data from {args.recordings_file}\")\n",
        "    else:\n",
        "        print(f\"---------------- Using data from {args.data_directory}\")\n",
        "\n",
        "    scores = calculate_dbres_scores(**vars(args))"
      ],
      "metadata": {
        "id": "1R4MjygcJZ8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: xgboost_integration"
      ],
      "metadata": {
        "id": "stf4feV8JlXu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.impute import SimpleImputer\n",
        "from tqdm import tqdm\n",
        "\n",
        "from DataProcessing.find_and_load_patient_files import (\n",
        "    find_patient_files,\n",
        "    load_patient_data,\n",
        ")\n",
        "from DataProcessing.helper_code import load_recordings\n",
        "from DataProcessing.label_extraction import get_murmur, get_outcome\n",
        "from DataProcessing.XGBoost_features.extract_all_features import extract_all_features\n",
        "from dbres import calculate_dbres_output\n",
        "from ModelEvaluation.evaluate_model import evaluate_model\n",
        "\n",
        "\n",
        "def get_murmurs_features(\n",
        "    model_name,\n",
        "    data_directory,\n",
        "    recalc_dbres_output,\n",
        "    dbres_output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    model_type,\n",
        "    bayesian\n",
        "):\n",
        "    patient_files = find_patient_files(data_directory)\n",
        "    num_patient_files = len(patient_files)\n",
        "\n",
        "    # Extract the features and labels.\n",
        "    if model_type == \"murmur\":\n",
        "        label_classes = [\"Present\", \"Unknown\", \"Absent\"]\n",
        "    elif model_type == \"murmur_binary\":\n",
        "        label_classes = [\"Present\", \"Absent\"]\n",
        "    elif model_type == \"outcome_binary\":\n",
        "        label_classes = [\"Abnormal\", \"Normal\"]\n",
        "    num_label_classes = len(label_classes)\n",
        "\n",
        "    features = list()\n",
        "    labels = list()\n",
        "    for i in range(num_patient_files):\n",
        "\n",
        "        # Load the current patient data and recordings.\n",
        "        current_patient_data = load_patient_data(patient_files[i])\n",
        "        current_recordings = load_recordings(data_directory, current_patient_data)\n",
        "        current_recordings = [r / 32768 for r in current_recordings]\n",
        "\n",
        "        # Extract features.\n",
        "        metadata_features, audio_features = extract_all_features(\n",
        "            current_patient_data, current_recordings\n",
        "        )\n",
        "        audio_features_reshaped = audio_features.reshape(1, -1)[0]\n",
        "        current_features = np.hstack((metadata_features, audio_features_reshaped))\n",
        "        features.append(current_features)\n",
        "\n",
        "        # Extract labels and use one-hot encoding.\n",
        "        current_label = np.zeros(num_label_classes, dtype=int)\n",
        "        if model_type == \"murmur\":\n",
        "            label = get_murmur(current_patient_data)\n",
        "        elif model_type == \"murmur_binary\":\n",
        "            label = get_murmur(current_patient_data)\n",
        "            if label == \"Unknown\":\n",
        "                label = \"Present\"\n",
        "        elif model_type == \"outcome_binary\":\n",
        "            label = get_outcome(current_patient_data)\n",
        "        if label in label_classes:\n",
        "            j = label_classes.index(label)\n",
        "            current_label[j] = 1\n",
        "        labels.append(current_label)\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    labels = np.vstack(labels)\n",
        "\n",
        "    imputer = SimpleImputer().fit(features)\n",
        "    features = imputer.transform(features)\n",
        "\n",
        "    _, spectrogram_outputs = calculate_dbres_output(\n",
        "        model_name,\n",
        "        recalc_dbres_output,\n",
        "        data_directory,\n",
        "        dbres_output_directory,\n",
        "        model_binary_pth,\n",
        "        model_binary_present_pth,\n",
        "        model_binary_unknown_pth,\n",
        "        bayesian\n",
        "    )\n",
        "\n",
        "    features_combined = np.vstack(\n",
        "        [np.concatenate((f, s)) for f, s in zip(features, spectrogram_outputs)]\n",
        "    )\n",
        "\n",
        "    return labels, features_combined\n",
        "\n",
        "\n",
        "def train_xgboost_integration(\n",
        "    model_name,\n",
        "    train_data_directory,\n",
        "    dbres_output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    model_type,\n",
        "    use_weights=False,\n",
        "    bayesian=True,\n",
        "):\n",
        "\n",
        "    if os.path.exists(dbres_output_directory):\n",
        "        recalculated_dbres_output = False\n",
        "    else:\n",
        "        recalculated_dbres_output = True\n",
        "        print(f\"DBRes output directory {dbres_output_directory} does not exist. Recalculating DBRes output.\")\n",
        "\n",
        "    murmurs, features_combined = get_murmurs_features(\n",
        "        model_name,\n",
        "        train_data_directory,\n",
        "        recalculated_dbres_output,\n",
        "        dbres_output_directory,\n",
        "        model_binary_pth,\n",
        "        model_binary_present_pth,\n",
        "        model_binary_unknown_pth,\n",
        "        model_type=model_type,\n",
        "        bayesian=bayesian\n",
        "    )\n",
        "\n",
        "    if use_weights:\n",
        "        w_pos = 5\n",
        "        print(f\"Using postivie class sample weight {w_pos}.\")\n",
        "        sample_weights = np.ones(len(murmurs))\n",
        "        for i in range(len(murmurs)):\n",
        "            if murmurs[i][0] == 1:\n",
        "                sample_weights[i] = w_pos\n",
        "    else:\n",
        "        print(\"Not using sample weights.\")\n",
        "        sample_weights = None\n",
        "\n",
        "    murmur_classifier = xgb.XGBClassifier()\n",
        "    murmur_classifier.fit(features_combined, murmurs, sample_weight = sample_weights)\n",
        "\n",
        "    return murmur_classifier\n",
        "\n",
        "\n",
        "def test_xgboost_integration(\n",
        "    model_name,\n",
        "    murmur_classifier,\n",
        "    test_data_directory,\n",
        "    dbres_output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    model_type,\n",
        "    recordings_file,\n",
        "    bayesian=True,\n",
        "):\n",
        "\n",
        "    if os.path.exists(dbres_output_directory):\n",
        "        recalculated_dbres_output = False\n",
        "    else:\n",
        "        recalculated_dbres_output = True\n",
        "        print(f\"DBRes output directory {dbres_output_directory} does not exist. Recalculating DBRes output.\")\n",
        "\n",
        "    # TODO: Add code to load the recordings file.\n",
        "    _, features_combined = get_murmurs_features(\n",
        "        model_name,\n",
        "        test_data_directory,\n",
        "        recalculated_dbres_output,\n",
        "        dbres_output_directory,\n",
        "        model_binary_pth,\n",
        "        model_binary_present_pth,\n",
        "        model_binary_unknown_pth,\n",
        "        model_type=model_type,\n",
        "        bayesian=bayesian\n",
        "    )\n",
        "\n",
        "    murmur_probabilities = murmur_classifier.predict_proba(features_combined)\n",
        "    murmur_outputs = np.zeros(murmur_probabilities.shape, dtype=np.int_)\n",
        "    idx = np.argmax(murmur_probabilities, axis=1)\n",
        "    for i in range(len(murmur_outputs)):\n",
        "        murmur_outputs[i][idx[i]] = 1\n",
        "\n",
        "    return murmur_probabilities, murmur_outputs\n",
        "\n",
        "\n",
        "def calculate_xgboost_integration_scores(\n",
        "    model_name,\n",
        "    train_data_directory,\n",
        "    test_data_directory,\n",
        "    model_xgb_pth,\n",
        "    dbres_output_directory,\n",
        "    model_binary_pth,\n",
        "    model_binary_present_pth,\n",
        "    model_binary_unknown_pth,\n",
        "    output_directory,\n",
        "    recordings_file,\n",
        "    use_weights,\n",
        "    bayesian\n",
        "):\n",
        "\n",
        "    if (model_binary_present_pth is not None) and (model_binary_unknown_pth is not None):\n",
        "        model_type = \"murmur\"\n",
        "    elif model_binary_pth is not None:\n",
        "        if \"MurmurBinary\" in model_binary_pth:\n",
        "            model_type = \"murmur_binary\"\n",
        "        elif \"OutcomeBinary\" in model_binary_pth:\n",
        "            model_type = \"outcome_binary\"\n",
        "        else:\n",
        "            raise Exception(\"No binary murmur or outcome model was provided.\")\n",
        "    else:\n",
        "        raise Exception(\"No model was provided.\")\n",
        "\n",
        "    print(f\"--- Using {model_type} model ---\")\n",
        "\n",
        "    # Train\n",
        "    if train_data_directory is not None:\n",
        "        print(\"Training the model.\")\n",
        "        murmur_classifier = train_xgboost_integration(\n",
        "            model_name,\n",
        "            train_data_directory,\n",
        "            os.path.join(dbres_output_directory, \"train\"),\n",
        "            model_binary_pth,\n",
        "            model_binary_present_pth,\n",
        "            model_binary_unknown_pth,\n",
        "            model_type=model_type,\n",
        "            use_weights=use_weights,\n",
        "            bayesian=bayesian\n",
        "        )\n",
        "        # Save the model.\n",
        "        if \"binary\" in model_type:\n",
        "            model_path = (\"/\").join(model_binary_pth.split(\"/\")[:-1]) + f\"/XGB_{model_type}.json\"\n",
        "        else:\n",
        "            model_path = (\"/\").join(model_binary_present_pth.split(\"/\")[:-1]) + f\"/XGB_{model_type}.json\"\n",
        "        murmur_classifier.save_model(model_path)\n",
        "        print(f\"Model saved to {model_path}\")\n",
        "    else:\n",
        "        print(\"No training data was provided. Loading the model.\")\n",
        "        assert model_xgb_pth is not None, \"No model or training data were provided.\"\n",
        "        murmur_classifier = xgb.XGBClassifier()\n",
        "        murmur_classifier.load_model(model_xgb_pth)\n",
        "\n",
        "    # Test\n",
        "    murmur_probabilities, murmur_outputs = test_xgboost_integration(\n",
        "        model_name,\n",
        "        murmur_classifier,\n",
        "        test_data_directory,\n",
        "        os.path.join(dbres_output_directory, \"test\"),\n",
        "        model_binary_pth,\n",
        "        model_binary_present_pth,\n",
        "        model_binary_unknown_pth,\n",
        "        model_type=model_type,\n",
        "        recordings_file = recordings_file,\n",
        "        bayesian=bayesian\n",
        "    )\n",
        "\n",
        "    if (model_binary_present_pth is not None) and (model_binary_unknown_pth is not None):\n",
        "        model_type = \"murmur\"\n",
        "    elif model_binary_pth is not None:\n",
        "        if \"MurmurBinary\" in model_binary_pth:\n",
        "            model_type = \"murmur_binary\"\n",
        "        elif \"OutcomeBinary\" in model_binary_pth:\n",
        "            model_type = \"outcome_binary\"\n",
        "        else:\n",
        "            raise Exception(\"No binary murmur or outcome model was provided.\")\n",
        "    else:\n",
        "        raise Exception(\"No model was provided.\")\n",
        "    print(f\"--- Evaluating {model_type} model ---\")\n",
        "    scores = evaluate_model(test_data_directory, murmur_probabilities, murmur_outputs, model_type=model_type, recordings_file = recordings_file, output_directory=output_directory)\n",
        "    print(\"---- XGBoost Integration Scores ----\")\n",
        "    print(f\"{scores}\")\n",
        "    with open(os.path.join(output_directory, \"DBRes_score.npy\"), \"w\") as text_file:\n",
        "        text_file.write(scores)\n",
        "\n",
        "    return scores\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(prog=\"XGBoostIntegration\")\n",
        "    parser.add_argument(\n",
        "        \"--model_name\",\n",
        "        type=str,\n",
        "        help=\"The ResNet to train. Current options are resnet50 or resnet50dropout.\",\n",
        "        choices=[\"resnet50\", \"resnet50dropout\"],\n",
        "        default=\"resnet50dropout\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--train_data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory of the training data.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--test_data_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory of the test data.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_xgb_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of the xgb model. Must be set if no training data are provided.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--dbres_output_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory in which DBRes's output will be saved.\",\n",
        "        default=\"data/dbres_outputs\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify present vs not present.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_present_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify present vs not present.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_binary_unknown_pth\",\n",
        "        type=str,\n",
        "        help=\"The path of binary ResNet trained to classify unknown vs not unknown.\",\n",
        "        default=None,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--output_directory\",\n",
        "        type=str,\n",
        "        help=\"The directory in which to save DBRes's output.\",\n",
        "        default=\"data/dbres_outputs\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--recordings_file\",\n",
        "        type=str,\n",
        "        help=\"The path to a recordings file.\",\n",
        "        default=\"\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--use_weights\",\n",
        "        type=bool,\n",
        "        help=\"Whether to use weights in the training data.\",\n",
        "        default=False,\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        '--disable-bayesian',\n",
        "        dest='bayesian',\n",
        "        action='store_false',\n",
        "        default=True,\n",
        "        help='Disable Bayesian features (default: Bayesian is enabled)'\n",
        "    )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(\"---------------- Starting xgboost_integration.py for training ----------------\")\n",
        "    print(f\"---------------- Using data from {args.train_data_directory}\")\n",
        "\n",
        "    scores = calculate_xgboost_integration_scores(**vars(args))"
      ],
      "metadata": {
        "id": "v4Tb-1u0Jly5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rTEnVw4WbqGk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uiy_i--xLtDzYB33NZ9Myiwqt8M-O1aA",
      "authorship_tag": "ABX9TyNm2jB6aphIt5q/I0NeOwAG"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}